# Vertex AI Integration Documentation

## ðŸ“‹ Overview

Vertex AI is Google Cloud's unified machine learning platform that powers EchoTune AI's generative capabilities. This comprehensive guide covers the complete integration of Vertex AI services for image generation, video creation, and multi-modal AI interactions.

## ðŸ—ï¸ Vertex AI Architecture

### Platform Components
- **Vertex AI Model Garden**: Access to foundation models
- **Imagen Models**: Advanced image generation (Imagen 2.0, 3.0)
- **Veo Models**: Video generation capabilities (Veo 1.5, 2.0)
- **Generative AI Studio**: Interactive model testing
- **Vertex AI Pipelines**: ML workflow orchestration

### EchoTune AI Integration Layer
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           EchoTune AI App           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       Generative AI Service        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       Vertex AI SDK Layer          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Google Cloud Platform         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Model Integration

### 1. Imagen Models (Image Generation)

#### Imagen 3.0 - Latest Generation
```python
from vertexai.preview.vision_models import ImageGenerationModel

# Initialize Imagen 3.0
imagen_3 = ImageGenerationModel.from_pretrained("imagen-3.0-generate-001")

# Generate high-resolution images
response = imagen_3.generate_images(
    prompt="Professional music studio with vintage equipment",
    number_of_images=1,
    aspect_ratio="16:9",
    safety_filter_level="block_most",
    person_generation="allow_adult"
)

# Features:
# - Resolution: Up to 1536x1536
# - Aspect ratios: 1:1, 16:9, 9:16, 4:3, 3:4
# - Cost: $0.025 per image
# - Safety filtering: Advanced content moderation
```

#### Imagen 2.0 - Balanced Performance
```python
# Initialize Imagen 2.0
imagen_2 = ImageGenerationModel.from_pretrained("imagegeneration@006")

# Generate standard images
response = imagen_2.generate_images(
    prompt="Abstract music visualization with flowing colors",
    number_of_images=4,
    guidance_scale=15,  # Higher values = more prompt adherence
    seed=42  # For reproducible results
)

# Features:
# - Resolution: 1024x1024
# - Batch generation: Up to 8 images
# - Cost: $0.020 per image
# - Fast processing: ~2-3 seconds
```

### 2. Veo Models (Video Generation)

#### Veo 2.0 - Premium Video Generation
```python
from vertexai.preview.generative_models import GenerativeModel

# Initialize Veo 2.0
veo_2 = GenerativeModel("veo-2.0-001")

# Generate high-quality videos
response = veo_2.generate_content(
    prompt="Dynamic music festival with colorful stage lighting",
    generation_config={
        "max_output_tokens": 8192,
        "temperature": 0.7,
        "video_length": 30,  # seconds
        "resolution": "4K",
        "fps": 30
    }
)

# Features:
# - Resolution: Up to 4K (3840x2160)
# - Duration: Up to 2 minutes
# - Cost: $0.025 per second
# - Quality: Cinematic with smooth motion
```

#### Veo 1.5 - Standard Video Generation
```python
# Initialize Veo 1.5
veo_1_5 = GenerativeModel("veo-1.5-001")

# Generate standard videos
response = veo_1_5.generate_content(
    prompt="Smooth album artwork animation with particle effects",
    generation_config={
        "video_length": 15,  # seconds
        "resolution": "1080p",
        "fps": 24,
        "style": "artistic"
    }
)

# Features:
# - Resolution: Up to 1080p (1920x1080)
# - Duration: Up to 1 minute
# - Cost: $0.015 per second
# - Fast generation: ~1.5s per video second
```

## ðŸ› ï¸ Implementation Guide

### 1. Service Configuration

```python
import os
import logging
from dataclasses import dataclass
from typing import List, Optional, Dict, Any
import vertexai
from vertexai.preview.vision_models import ImageGenerationModel
from vertexai.preview.generative_models import GenerativeModel
from google.cloud import aiplatform

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class VertexAIConfig:
    """Vertex AI configuration settings."""
    project_id: str
    location: str
    credentials_path: Optional[str] = None
    staging_bucket: Optional[str] = None

class VertexAIService:
    """Production-ready Vertex AI service integration."""
    
    def __init__(self, config: VertexAIConfig):
        self.config = config
        self._initialize_clients()
        self._load_models()
    
    def _initialize_clients(self):
        """Initialize Vertex AI clients and authentication."""
        try:
            # Initialize Vertex AI
            vertexai.init(
                project=self.config.project_id,
                location=self.config.location,
                staging_bucket=self.config.staging_bucket
            )
            
            # Initialize AI Platform
            aiplatform.init(
                project=self.config.project_id,
                location=self.config.location
            )
            
            logger.info("âœ… Vertex AI clients initialized successfully")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize Vertex AI: {e}")
            raise
    
    def _load_models(self):
        """Load and cache model instances."""
        try:
            # Image generation models
            self.imagen_3 = ImageGenerationModel.from_pretrained("imagen-3.0-generate-001")
            self.imagen_2 = ImageGenerationModel.from_pretrained("imagegeneration@006")
            
            # Video generation models
            self.veo_2 = GenerativeModel("veo-2.0-001")
            self.veo_1_5 = GenerativeModel("veo-1.5-001")
            
            # Text generation models
            self.gemini_pro = GenerativeModel("gemini-pro")
            self.gemini_pro_vision = GenerativeModel("gemini-pro-vision")
            
            logger.info("âœ… All models loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ Failed to load models: {e}")
            raise
```

### 2. Image Generation Implementation

```python
from enum import Enum
from dataclasses import dataclass
from typing import List, Optional
import hashlib
import time

class ImageStyle(Enum):
    PHOTOGRAPHIC = "photographic"
    DIGITAL_ART = "digital-art"
    CINEMATIC = "cinematic"
    ANIME = "anime"
    SKETCH = "sketch"

class AspectRatio(Enum):
    SQUARE = "1:1"
    LANDSCAPE = "16:9"
    PORTRAIT = "9:16"
    PHOTO = "4:3"
    PORTRAIT_PHOTO = "3:4"

@dataclass
class ImageGenerationRequest:
    prompt: str
    model: str = "imagen-3.0-generate-001"
    count: int = 1
    aspect_ratio: AspectRatio = AspectRatio.SQUARE
    style: Optional[ImageStyle] = None
    negative_prompt: Optional[str] = None
    guidance_scale: float = 7.5
    seed: Optional[int] = None

class ImageGenerator:
    """Advanced image generation with Vertex AI."""
    
    def __init__(self, vertex_service: VertexAIService):
        self.vertex_service = vertex_service
        self.generation_cache = {}
    
    async def generate_images(self, request: ImageGenerationRequest) -> List[str]:
        """Generate images with caching and error handling."""
        start_time = time.time()
        
        try:
            # Check cache first
            cache_key = self._create_cache_key(request)
            if cache_key in self.generation_cache:
                logger.info("ðŸ“‹ Using cached generation result")
                return self.generation_cache[cache_key]
            
            # Select model
            model = self._select_model(request.model)
            
            # Prepare generation parameters
            generation_params = self._prepare_params(request)
            
            # Generate images
            logger.info(f"ðŸŽ¨ Generating {request.count} images with {request.model}")
            response = model.generate_images(**generation_params)
            
            # Process results
            image_paths = await self._process_response(response, request)
            
            # Cache results
            self.generation_cache[cache_key] = image_paths
            
            generation_time = time.time() - start_time
            logger.info(f"âœ… Generated {len(image_paths)} images in {generation_time:.2f}s")
            
            return image_paths
            
        except Exception as e:
            logger.error(f"âŒ Image generation failed: {e}")
            raise
    
    def _select_model(self, model_name: str):
        """Select appropriate model based on request."""
        model_map = {
            "imagen-3.0-generate-001": self.vertex_service.imagen_3,
            "imagegeneration@006": self.vertex_service.imagen_2,
            "imagen-2.0": self.vertex_service.imagen_2
        }
        
        if model_name not in model_map:
            logger.warning(f"âš ï¸ Unknown model {model_name}, using Imagen 3.0")
            return self.vertex_service.imagen_3
        
        return model_map[model_name]
    
    def _prepare_params(self, request: ImageGenerationRequest) -> Dict[str, Any]:
        """Prepare generation parameters."""
        params = {
            "prompt": request.prompt,
            "number_of_images": request.count,
            "aspect_ratio": request.aspect_ratio.value,
            "safety_filter_level": "block_most"
        }
        
        if request.style:
            params["add_watermark"] = False
            params["guidance_scale"] = request.guidance_scale
        
        if request.negative_prompt:
            params["negative_prompt"] = request.negative_prompt
        
        if request.seed:
            params["seed"] = request.seed
        
        return params
    
    def _create_cache_key(self, request: ImageGenerationRequest) -> str:
        """Create cache key for request."""
        cache_data = f"{request.prompt}_{request.model}_{request.aspect_ratio.value}_{request.style}"
        return hashlib.md5(cache_data.encode()).hexdigest()
```

### 3. Video Generation Implementation

```python
@dataclass
class VideoGenerationRequest:
    prompt: str
    model: str = "veo-2.0-001"
    duration_seconds: int = 10
    resolution: str = "1080p"  # 1080p, 4K
    fps: int = 24
    style: Optional[str] = None
    seed_image: Optional[str] = None

class VideoGenerator:
    """Advanced video generation with Vertex AI."""
    
    def __init__(self, vertex_service: VertexAIService):
        self.vertex_service = vertex_service
    
    async def generate_video(self, request: VideoGenerationRequest) -> str:
        """Generate video with comprehensive error handling."""
        start_time = time.time()
        
        try:
            # Select model
            model = self._select_model(request.model)
            
            # Prepare generation config
            generation_config = self._prepare_video_config(request)
            
            # Generate video
            logger.info(f"ðŸŽ¬ Generating {request.duration_seconds}s video with {request.model}")
            
            if request.seed_image:
                # Image-to-video generation
                response = await self._generate_from_image(model, request, generation_config)
            else:
                # Text-to-video generation
                response = await self._generate_from_text(model, request, generation_config)
            
            # Process and save video
            video_path = await self._process_video_response(response, request)
            
            generation_time = time.time() - start_time
            logger.info(f"âœ… Generated video in {generation_time:.2f}s")
            
            return video_path
            
        except Exception as e:
            logger.error(f"âŒ Video generation failed: {e}")
            raise
    
    def _select_model(self, model_name: str):
        """Select video generation model."""
        model_map = {
            "veo-2.0-001": self.vertex_service.veo_2,
            "veo-1.5-001": self.vertex_service.veo_1_5
        }
        
        if model_name not in model_map:
            logger.warning(f"âš ï¸ Unknown model {model_name}, using Veo 2.0")
            return self.vertex_service.veo_2
        
        return model_map[model_name]
    
    def _prepare_video_config(self, request: VideoGenerationRequest) -> Dict[str, Any]:
        """Prepare video generation configuration."""
        config = {
            "max_output_tokens": 8192,
            "temperature": 0.7,
            "video_length": request.duration_seconds,
            "fps": request.fps
        }
        
        # Resolution mapping
        resolution_map = {
            "1080p": "1920x1080",
            "4K": "3840x2160",
            "720p": "1280x720"
        }
        
        config["resolution"] = resolution_map.get(request.resolution, "1920x1080")
        
        if request.style:
            config["style"] = request.style
        
        return config
```

## ðŸ“Š Performance Optimization

### 1. Model Selection Strategy

```python
class ModelSelector:
    """Intelligent model selection based on requirements."""
    
    @staticmethod
    def select_image_model(prompt_length: int, quality_priority: bool = True, 
                          cost_priority: bool = False) -> str:
        """Select optimal image generation model."""
        
        if cost_priority:
            return "imagegeneration@006"  # Imagen 2.0 - Lower cost
        
        if quality_priority and prompt_length > 100:
            return "imagen-3.0-generate-001"  # Imagen 3.0 - Best quality
        
        return "imagegeneration@006"  # Balanced choice
    
    @staticmethod
    def select_video_model(duration: int, quality_priority: bool = True) -> str:
        """Select optimal video generation model."""
        
        if duration > 60 or quality_priority:
            return "veo-2.0-001"  # Veo 2.0 - Premium
        
        return "veo-1.5-001"  # Veo 1.5 - Standard
```

### 2. Batch Processing

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class BatchProcessor:
    """Efficient batch processing for multiple generations."""
    
    def __init__(self, max_workers: int = 5):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def batch_generate_images(self, requests: List[ImageGenerationRequest]) -> List[List[str]]:
        """Process multiple image generation requests concurrently."""
        
        # Create semaphore to limit concurrent requests
        semaphore = asyncio.Semaphore(self.max_workers)
        
        async def generate_with_limit(request):
            async with semaphore:
                return await self.image_generator.generate_images(request)
        
        # Execute all requests concurrently
        tasks = [generate_with_limit(req) for req in requests]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter successful results
        successful_results = [r for r in results if not isinstance(r, Exception)]
        failed_count = len(results) - len(successful_results)
        
        if failed_count > 0:
            logger.warning(f"âš ï¸ {failed_count} generations failed")
        
        return successful_results
```

### 3. Cost Optimization

```python
class CostOptimizer:
    """Advanced cost optimization strategies."""
    
    # Model pricing (per unit)
    MODEL_COSTS = {
        "imagen-3.0-generate-001": 0.025,  # per image
        "imagegeneration@006": 0.020,      # per image
        "veo-2.0-001": 0.025,             # per second
        "veo-1.5-001": 0.015              # per second
    }
    
    @classmethod
    def calculate_request_cost(cls, request) -> float:
        """Calculate cost for a generation request."""
        model_cost = cls.MODEL_COSTS.get(request.model, 0.025)
        
        if hasattr(request, 'count'):  # Image request
            return model_cost * request.count
        elif hasattr(request, 'duration_seconds'):  # Video request
            return model_cost * request.duration_seconds
        
        return model_cost
    
    @classmethod
    def optimize_batch_cost(cls, requests: List) -> List:
        """Optimize batch requests for cost efficiency."""
        # Sort by cost efficiency (quality/cost ratio)
        def cost_efficiency(request):
            cost = cls.calculate_request_cost(request)
            # Simple quality heuristic based on model
            quality_score = 1.0 if "3.0" in request.model or "2.0" in request.model else 0.8
            return quality_score / cost if cost > 0 else 0
        
        return sorted(requests, key=cost_efficiency, reverse=True)
    
    @classmethod
    def suggest_model_alternative(cls, request, budget_limit: float) -> str:
        """Suggest alternative model within budget."""
        current_cost = cls.calculate_request_cost(request)
        
        if current_cost > budget_limit:
            # Suggest cheaper alternatives
            if "imagen-3.0" in request.model:
                return "imagegeneration@006"  # Cheaper image model
            elif "veo-2.0" in request.model:
                return "veo-1.5-001"  # Cheaper video model
        
        return request.model
```

## ðŸ” Quality Control & Safety

### 1. Content Safety Filters

```python
class SafetyFilter:
    """Comprehensive content safety filtering."""
    
    BLOCKED_KEYWORDS = [
        'violence', 'explicit', 'harmful', 'illegal', 'nsfw',
        'weapon', 'drug', 'hate', 'discrimination'
    ]
    
    @classmethod
    def validate_prompt(cls, prompt: str) -> tuple[bool, str]:
        """Validate prompt for safety compliance."""
        prompt_lower = prompt.lower()
        
        # Check for blocked keywords
        for keyword in cls.BLOCKED_KEYWORDS:
            if keyword in prompt_lower:
                return False, f"Prompt contains blocked keyword: {keyword}"
        
        # Check prompt length
        if len(prompt) > 1000:
            return False, "Prompt too long (max 1000 characters)"
        
        if len(prompt.strip()) < 5:
            return False, "Prompt too short (min 5 characters)"
        
        return True, "Prompt validated successfully"
    
    @classmethod
    def apply_safety_settings(cls, model_params: Dict[str, Any]) -> Dict[str, Any]:
        """Apply comprehensive safety settings."""
        safety_settings = {
            "safety_filter_level": "block_most",
            "person_generation": "allow_adult",
            "add_watermark": True,
            "block_nsfw": True
        }
        
        model_params.update(safety_settings)
        return model_params
```

### 2. Quality Validation

```python
from PIL import Image
import cv2
import numpy as np

class QualityValidator:
    """Validate generated content quality."""
    
    @staticmethod
    def validate_image_quality(image_path: str) -> Dict[str, Any]:
        """Analyze image quality metrics."""
        try:
            with Image.open(image_path) as img:
                # Basic metrics
                width, height = img.size
                aspect_ratio = width / height
                file_size = os.path.getsize(image_path)
                
                # Convert to array for analysis
                img_array = np.array(img)
                
                # Calculate sharpness (Laplacian variance)
                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
                sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
                
                # Calculate brightness
                brightness = np.mean(img_array)
                
                # Calculate contrast
                contrast = np.std(img_array)
                
                return {
                    "width": width,
                    "height": height,
                    "aspect_ratio": aspect_ratio,
                    "file_size_kb": file_size / 1024,
                    "sharpness_score": sharpness,
                    "brightness_score": brightness,
                    "contrast_score": contrast,
                    "quality_grade": cls._calculate_quality_grade(sharpness, brightness, contrast)
                }
                
        except Exception as e:
            logger.error(f"Quality validation failed: {e}")
            return {"error": str(e)}
    
    @staticmethod
    def _calculate_quality_grade(sharpness: float, brightness: float, contrast: float) -> str:
        """Calculate overall quality grade."""
        score = 0
        
        # Sharpness scoring
        if sharpness > 1000: score += 3
        elif sharpness > 500: score += 2
        elif sharpness > 100: score += 1
        
        # Brightness scoring (optimal range: 100-200)
        if 100 <= brightness <= 200: score += 2
        elif 50 <= brightness <= 250: score += 1
        
        # Contrast scoring
        if contrast > 50: score += 2
        elif contrast > 25: score += 1
        
        # Grade mapping
        if score >= 6: return "A"
        elif score >= 4: return "B"
        elif score >= 2: return "C"
        else: return "D"
```

## ðŸš¨ Error Handling & Recovery

### 1. Comprehensive Error Management

```python
from google.api_core import exceptions as gcp_exceptions
import backoff

class VertexAIErrorHandler:
    """Advanced error handling for Vertex AI operations."""
    
    @staticmethod
    @backoff.on_exception(
        backoff.expo,
        (gcp_exceptions.ResourceExhausted, gcp_exceptions.ServiceUnavailable),
        max_tries=3,
        max_time=300
    )
    async def robust_generate(generation_func, *args, **kwargs):
        """Execute generation with automatic retry on transient errors."""
        try:
            return await generation_func(*args, **kwargs)
            
        except gcp_exceptions.PermissionDenied as e:
            logger.error(f"ðŸ”’ Permission denied: {e}")
            raise ValueError("Insufficient permissions for Vertex AI access")
            
        except gcp_exceptions.ResourceExhausted as e:
            logger.warning(f"âš ï¸ Quota exceeded: {e}")
            # Implement intelligent backoff
            await asyncio.sleep(60)  # Wait 1 minute
            raise  # Will be retried by backoff decorator
            
        except gcp_exceptions.InvalidArgument as e:
            logger.error(f"âŒ Invalid request: {e}")
            raise ValueError(f"Invalid generation parameters: {e}")
            
        except gcp_exceptions.NotFound as e:
            logger.error(f"ðŸ” Model not found: {e}")
            raise ValueError("Requested model is not available")
            
        except Exception as e:
            logger.error(f"ðŸ’¥ Unexpected error: {e}")
            raise RuntimeError(f"Generation failed: {e}")
    
    @staticmethod
    def handle_quota_exceeded():
        """Handle quota exceeded scenarios."""
        return {
            "status": "quota_exceeded",
            "message": "Generation quota exceeded. Please try again later.",
            "retry_after": 3600,  # 1 hour
            "suggested_action": "Consider upgrading your quota or using a different model"
        }
```

### 2. Fallback Strategies

```python
class FallbackManager:
    """Intelligent fallback strategies for model failures."""
    
    MODEL_FALLBACKS = {
        "imagen-3.0-generate-001": "imagegeneration@006",
        "veo-2.0-001": "veo-1.5-001"
    }
    
    async def generate_with_fallback(self, request, primary_generator):
        """Attempt generation with fallback models."""
        try:
            # Try primary model
            return await primary_generator(request)
            
        except (gcp_exceptions.ResourceExhausted, gcp_exceptions.ServiceUnavailable):
            # Try fallback model
            fallback_model = self.MODEL_FALLBACKS.get(request.model)
            
            if fallback_model:
                logger.info(f"ðŸ”„ Falling back to {fallback_model}")
                request.model = fallback_model
                return await primary_generator(request)
            
            raise
        
        except Exception as e:
            logger.error(f"âŒ All generation attempts failed: {e}")
            raise
```

## ðŸ“ˆ Monitoring & Analytics

### 1. Performance Tracking

```python
import time
from dataclasses import dataclass
from typing import Dict, List
import json

@dataclass
class GenerationMetrics:
    """Comprehensive generation metrics."""
    request_id: str
    model: str
    start_time: float
    end_time: float
    success: bool
    cost: float
    prompt_length: int
    output_count: int
    error_message: Optional[str] = None

class MetricsCollector:
    """Collect and analyze generation metrics."""
    
    def __init__(self):
        self.metrics: List[GenerationMetrics] = []
    
    def record_generation(self, metrics: GenerationMetrics):
        """Record generation metrics."""
        self.metrics.append(metrics)
        
        # Log key metrics
        duration = metrics.end_time - metrics.start_time
        status = "âœ…" if metrics.success else "âŒ"
        
        logger.info(f"{status} {metrics.model}: {duration:.2f}s, ${metrics.cost:.4f}")
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Generate comprehensive performance summary."""
        if not self.metrics:
            return {"error": "No metrics available"}
        
        successful = [m for m in self.metrics if m.success]
        failed = [m for m in self.metrics if not m.success]
        
        total_duration = sum(m.end_time - m.start_time for m in self.metrics)
        total_cost = sum(m.cost for m in successful)
        
        return {
            "total_requests": len(self.metrics),
            "successful_requests": len(successful),
            "failed_requests": len(failed),
            "success_rate": len(successful) / len(self.metrics) * 100,
            "total_duration_seconds": total_duration,
            "average_duration": total_duration / len(self.metrics),
            "total_cost_usd": total_cost,
            "average_cost_per_request": total_cost / len(successful) if successful else 0,
            "models_used": list(set(m.model for m in self.metrics)),
            "performance_by_model": self._get_model_performance()
        }
    
    def _get_model_performance(self) -> Dict[str, Dict[str, float]]:
        """Analyze performance by model."""
        model_stats = {}
        
        for model in set(m.model for m in self.metrics):
            model_metrics = [m for m in self.metrics if m.model == model]
            successful = [m for m in model_metrics if m.success]
            
            if model_metrics:
                avg_duration = sum(m.end_time - m.start_time for m in model_metrics) / len(model_metrics)
                success_rate = len(successful) / len(model_metrics) * 100
                avg_cost = sum(m.cost for m in successful) / len(successful) if successful else 0
                
                model_stats[model] = {
                    "requests": len(model_metrics),
                    "success_rate": success_rate,
                    "average_duration": avg_duration,
                    "average_cost": avg_cost
                }
        
        return model_stats
```

### 2. Real-time Monitoring

```python
from google.cloud import monitoring_v3
import time

class VertexAIMonitor:
    """Real-time monitoring for Vertex AI operations."""
    
    def __init__(self, project_id: str):
        self.project_id = project_id
        self.client = monitoring_v3.MetricServiceClient()
        self.project_name = f"projects/{project_id}"
    
    def create_custom_metrics(self):
        """Create custom Cloud Monitoring metrics."""
        metrics = [
            {
                "type": "custom.googleapis.com/vertex_ai/generation_requests",
                "description": "Number of Vertex AI generation requests"
            },
            {
                "type": "custom.googleapis.com/vertex_ai/generation_latency",
                "description": "Vertex AI generation latency in seconds"
            },
            {
                "type": "custom.googleapis.com/vertex_ai/generation_cost",
                "description": "Vertex AI generation cost in USD"
            }
        ]
        
        for metric in metrics:
            descriptor = monitoring_v3.MetricDescriptor()
            descriptor.type = metric["type"]
            descriptor.metric_kind = monitoring_v3.MetricDescriptor.MetricKind.GAUGE
            descriptor.value_type = monitoring_v3.MetricDescriptor.ValueType.DOUBLE
            descriptor.description = metric["description"]
            
            try:
                self.client.create_metric_descriptor(
                    name=self.project_name, 
                    metric_descriptor=descriptor
                )
                logger.info(f"âœ… Created metric: {metric['type']}")
            except Exception as e:
                logger.warning(f"âš ï¸ Metric creation failed: {e}")
    
    def record_metric(self, metric_type: str, value: float, labels: Dict[str, str] = None):
        """Record a custom metric value."""
        try:
            series = monitoring_v3.TimeSeries()
            series.metric.type = f"custom.googleapis.com/vertex_ai/{metric_type}"
            
            if labels:
                for key, val in labels.items():
                    series.metric.labels[key] = val
            
            series.resource.type = "global"
            
            now = time.time()
            seconds = int(now)
            nanos = int((now - seconds) * 10 ** 9)
            interval = monitoring_v3.TimeInterval({
                "end_time": {"seconds": seconds, "nanos": nanos}
            })
            point = monitoring_v3.Point({
                "interval": interval,
                "value": {"double_value": value}
            })
            series.points = [point]
            
            self.client.create_time_series(
                name=self.project_name, 
                time_series=[series]
            )
            
        except Exception as e:
            logger.error(f"âŒ Failed to record metric: {e}")
```

## ðŸŽ¯ Best Practices

### 1. Prompt Engineering

```python
class PromptOptimizer:
    """Advanced prompt optimization for better results."""
    
    STYLE_MODIFIERS = {
        "photographic": "professional photography, high resolution, detailed",
        "artistic": "artistic style, creative composition, vibrant colors",
        "cinematic": "cinematic lighting, dramatic composition, film-like quality",
        "minimalist": "clean, simple, minimal design, elegant composition"
    }
    
    QUALITY_ENHANCERS = [
        "high quality", "detailed", "sharp focus", "professional",
        "8k resolution", "masterpiece", "best quality"
    ]
    
    @classmethod
    def optimize_prompt(cls, base_prompt: str, style: str = "photographic", 
                       enhance_quality: bool = True) -> str:
        """Optimize prompt for better generation results."""
        
        # Start with base prompt
        optimized = base_prompt.strip()
        
        # Add style modifiers
        if style in cls.STYLE_MODIFIERS:
            optimized += f", {cls.STYLE_MODIFIERS[style]}"
        
        # Add quality enhancers
        if enhance_quality:
            quality_terms = ", ".join(cls.QUALITY_ENHANCERS[:3])
            optimized += f", {quality_terms}"
        
        # Clean up formatting
        optimized = ", ".join(term.strip() for term in optimized.split(",") if term.strip())
        
        return optimized
    
    @classmethod
    def create_negative_prompt(cls, avoid_terms: List[str] = None) -> str:
        """Create effective negative prompt."""
        default_negative = [
            "blurry", "low quality", "pixelated", "distorted",
            "watermark", "text", "logo", "signature"
        ]
        
        if avoid_terms:
            default_negative.extend(avoid_terms)
        
        return ", ".join(default_negative)
```

### 2. Resource Management

```python
import asyncio
from contextlib import asynccontextmanager

class ResourceManager:
    """Efficient resource management for Vertex AI operations."""
    
    def __init__(self, max_concurrent: int = 10):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.active_requests = 0
        self.request_queue = asyncio.Queue()
    
    @asynccontextmanager
    async def acquire_slot(self):
        """Acquire a generation slot with resource limiting."""
        async with self.semaphore:
            self.active_requests += 1
            try:
                yield
            finally:
                self.active_requests -= 1
    
    async def queue_generation(self, generation_func, *args, **kwargs):
        """Queue generation request with intelligent scheduling."""
        async with self.acquire_slot():
            return await generation_func(*args, **kwargs)
    
    def get_resource_status(self) -> Dict[str, int]:
        """Get current resource usage status."""
        return {
            "active_requests": self.active_requests,
            "available_slots": self.semaphore._value,
            "queue_size": self.request_queue.qsize()
        }
```

---

## ðŸ”„ Migration & Scaling

### Production Deployment Checklist

- [ ] **Authentication**: Service account configured with minimal permissions
- [ ] **Monitoring**: Custom metrics and alerting configured
- [ ] **Cost Controls**: Budget alerts and quota monitoring
- [ ] **Error Handling**: Comprehensive retry logic and fallbacks
- [ ] **Performance**: Caching and batch processing optimized
- [ ] **Security**: Content filtering and safety measures
- [ ] **Documentation**: All integrations documented and tested

### Scaling Considerations

1. **Horizontal Scaling**: Use multiple instances with load balancing
2. **Regional Deployment**: Deploy across multiple GCP regions
3. **Caching Strategy**: Implement Redis/Memcached for response caching
4. **Queue Management**: Use Cloud Tasks for request queuing
5. **Auto-scaling**: Configure based on request volume and latency

---

**Last Updated**: January 2025  
**Integration Status**: Production Ready âœ…  
**Performance**: Optimized for Enterprise Scale ðŸš€