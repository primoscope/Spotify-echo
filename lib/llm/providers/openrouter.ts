import type { LlmMessage, LlmProvider, LlmGenerateOptions } from '../types';

interface OpenRouterModelMeta { id: string; pricing?: Record<string, number | null | undefined>; }
const OPENROUTER_BASE = 'https://openrouter.ai/api/v1';
let cachedModels: OpenRouterModelMeta[] | null = null; let lastFetch = 0;
function isFree(m: OpenRouterModelMeta): boolean { if (!m.pricing) return true; return Object.values(m.pricing).every(v => v == null || v === 0); }
async function fetchModels(apiKey: string): Promise<OpenRouterModelMeta[]> { const now = Date.now(); if (cachedModels && (now - lastFetch) < 5 * 60 * 1000) return cachedModels; const res = await fetch(`${OPENROUTER_BASE}/models`, { headers: { Authorization: `Bearer ${apiKey}`, 'Content-Type': 'application/json' } }); if (!res.ok) throw new Error(`OpenRouter models fetch failed: ${res.status}`); const data = await res.json(); cachedModels = (data.data || []).map((d: any) => ({ id: d.id, pricing: d.pricing })); lastFetch = now; return cachedModels!; }
export class OpenRouterProvider implements LlmProvider { name = 'openrouter'; private model: string; private readonly allowed: Set<string>; private readonly freeOnly: boolean; private readonly strict: boolean; private readonly apiKey: string; private readonly timeoutMs: number; constructor() { this.apiKey = process.env.OPENROUTER_API_KEY || ''; if (!this.apiKey) throw new Error('OPENROUTER_API_KEY missing'); this.model = process.env.LLM_MODEL || 'meta-llama/llama-3-8b-instruct'; this.allowed = new Set((process.env.LLM_ALLOWED_MODELS || this.model).split(',').map(s => s.trim()).filter(Boolean)); this.freeOnly = (process.env.LLM_FREE_ONLY || 'false').toLowerCase() === 'true'; this.strict = (process.env.LLM_ENFORCE_FREE_STRICT || 'false').toLowerCase() === 'true'; this.timeoutMs = parseInt(process.env.LLM_TIMEOUT_MS || '20000', 10); } getActiveModel() { return this.model; } private async ensureModelCompliance() { if (!this.allowed.has(this.model)) { const first = [...this.allowed][0]; console.warn(`[LLM] Model ${this.model} not in allowed list. Falling back to ${first}`); this.model = first; } if (this.freeOnly) { try { const models = await fetchModels(this.apiKey); const currentMeta = models.find(m => m.id === this.model); if (!currentMeta || !isFree(currentMeta)) { const freeAllowed = models.filter(m => this.allowed.has(m.id) && isFree(m)); if (!freeAllowed.length) { const msg = '[LLM] No free allowed models available.'; if (this.strict) throw new Error(msg); console.warn(msg); return; } console.warn(`[LLM] Model ${this.model} not free. Falling back to ${freeAllowed[0].id}`); this.model = freeAllowed[0].id; } } catch (e) { if (this.strict) throw e; console.warn('[LLM] Free model enforcement check failed:', e); } } } async generate(messages: LlmMessage[], options?: LlmGenerateOptions): Promise<string> { await this.ensureModelCompliance(); const model = options?.model || this.model; const controller = new AbortController(); const timeout = setTimeout(() => controller.abort(), options?.timeoutMs || this.timeoutMs); try { const res = await fetch(`${OPENROUTER_BASE}/chat/completions`, { method: 'POST', headers: { Authorization: `Bearer ${this.apiKey}`, 'Content-Type': 'application/json', ...(process.env.NEXT_PUBLIC_APP_URL ? {'HTTP-Referer': process.env.NEXT_PUBLIC_APP_URL} : {}), 'X-Title': 'Spotify Echo' }, body: JSON.stringify({ model, messages }), signal: controller.signal }); if (!res.ok) { const text = await res.text(); throw new Error(`OpenRouter error ${res.status}: ${text.slice(0,300)}`); } const data = await res.json(); return data.choices?.[0]?.message?.content || ''; } finally { clearTimeout(timeout); } } }