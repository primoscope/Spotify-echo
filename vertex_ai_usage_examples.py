#!/usr/bin/env python3
"""
Quick Usage Example for Vertex AI Integration

This script shows how easy it is to use the Vertex AI integration
in a real application context within EchoTune AI.
"""

import asyncio
import os
import sys

# Add project root to path
sys.path.insert(0, os.path.dirname(__file__))

# Mock environment for demo
os.environ.setdefault('GCP_PROJECT_ID', 'echotune-ai-production')
os.environ.setdefault('GCP_REGION', 'us-central1')

from src.services.vertex_ai_service import VertexAIService, ModelRequest


async def analyze_user_music_preferences():
    """Example: Use Claude Opus to analyze user music preferences."""
    service = VertexAIService()
    
    # Simulate user listening history data
    user_data = {
        'recent_tracks': ['Bohemian Rhapsody', 'Hotel California', 'Stairway to Heaven'],
        'top_genres': ['Rock', 'Progressive Rock', 'Classic Rock'],
        'listening_time': 'Evening',
        'mood_preference': 'Contemplative'
    }
    
    prompt = f"""
    As a music recommendation expert, analyze this user's listening patterns:
    
    Recent tracks: {', '.join(user_data['recent_tracks'])}
    Top genres: {', '.join(user_data['top_genres'])}
    Listening time: {user_data['listening_time']}
    Mood preference: {user_data['mood_preference']}
    
    Provide 3 specific song recommendations with detailed reasoning.
    """
    
    request = ModelRequest(
        model_id="claude-opus-4-1@20250805",
        prompt=prompt,
        max_tokens=1000,
        temperature=0.7
    )
    
    print("üéµ Using Claude Opus for music recommendation analysis...")
    # In production, this would make a real API call
    print(f"üìù Prompt: {prompt[:100]}...")
    print("‚úÖ Analysis complete (simulated)")
    
    return {
        'model': 'Claude Opus 4.1',
        'analysis': 'Sophisticated pattern recognition and recommendation logic',
        'recommendations': [
            'Pink Floyd - Comfortably Numb',
            'Led Zeppelin - Kashmir', 
            'Yes - Roundabout'
        ]
    }


async def optimize_playlist_with_gemini():
    """Example: Use Gemini for playlist optimization."""
    service = VertexAIService()
    
    playlist_data = {
        'current_tracks': 15,
        'duration_minutes': 45,
        'energy_level': 'Medium',
        'user_feedback': 'Too many slow songs'
    }
    
    prompt = f"""
    Optimize this playlist based on user feedback:
    
    Current playlist: {playlist_data['current_tracks']} tracks, {playlist_data['duration_minutes']} minutes
    Energy level: {playlist_data['energy_level']}
    User feedback: {playlist_data['user_feedback']}
    
    Suggest specific improvements and track replacements.
    """
    
    request = ModelRequest(
        model_id="gemini-2.5-pro",
        prompt=prompt,
        max_tokens=800,
        streaming=True
    )
    
    print("üéß Using Gemini Pro for playlist optimization...")
    print(f"üìù Prompt: {prompt[:100]}...")
    print("‚úÖ Optimization complete (simulated)")
    
    return {
        'model': 'Gemini 2.5 Pro',
        'optimization': 'Real-time playlist balancing and track suggestions',
        'changes': [
            'Replace 3 slow tracks with medium-energy alternatives',
            'Add 2 upbeat transition tracks',
            'Reorder for better flow'
        ]
    }


async def real_time_chat_with_gemini_flash():
    """Example: Use Gemini Flash for real-time music chat."""
    service = VertexAIService()
    
    user_message = "I'm feeling nostalgic tonight. What should I listen to?"
    
    request = ModelRequest(
        model_id="gemini-2.5-flash",
        prompt=f"User says: '{user_message}'. Respond as a knowledgeable music companion.",
        max_tokens=300,
        streaming=True
    )
    
    print("üí¨ Using Gemini Flash for real-time chat...")
    print(f"üë§ User: {user_message}")
    print("ü§ñ Assistant: Generating response...")
    print("‚úÖ Response ready (simulated)")
    
    return {
        'model': 'Gemini 2.5 Flash',
        'response': 'Empathetic and contextual music recommendations',
        'speed': 'Optimized for real-time conversation'
    }


async def demonstrate_unified_interface():
    """Show how the unified interface works across all models."""
    service = VertexAIService()
    
    print("\nüöÄ Vertex AI Integration - Unified Interface Demo")
    print("=" * 60)
    
    # Example 1: Claude for deep analysis
    claude_result = await analyze_user_music_preferences()
    print(f"\n‚úÖ {claude_result['model']}: {claude_result['analysis']}")
    
    # Example 2: Gemini Pro for optimization
    gemini_result = await optimize_playlist_with_gemini()
    print(f"\n‚úÖ {gemini_result['model']}: {gemini_result['optimization']}")
    
    # Example 3: Gemini Flash for real-time interaction
    flash_result = await real_time_chat_with_gemini_flash()
    print(f"\n‚úÖ {flash_result['model']}: {flash_result['response']}")
    
    print("\nüéØ Integration Benefits:")
    print("   ‚Ä¢ Single interface for all models")
    print("   ‚Ä¢ Automatic model routing based on use case")
    print("   ‚Ä¢ Production-ready error handling")
    print("   ‚Ä¢ Built-in performance monitoring")
    
    print("\nüìä Model Comparison:")
    print("   ‚Ä¢ Claude Opus: Best for complex analysis and reasoning")
    print("   ‚Ä¢ Gemini Pro: Excellent for Google Cloud integration")
    print("   ‚Ä¢ Gemini Flash: Optimal for real-time interactions")
    
    return {
        'claude': claude_result,
        'gemini_pro': gemini_result,
        'gemini_flash': flash_result,
        'status': 'All models working seamlessly'
    }


if __name__ == "__main__":
    print("üéµ EchoTune AI - Vertex AI Integration Usage Examples")
    print("=" * 60)
    print("This demonstrates how the Vertex AI integration works in practice")
    print("within the EchoTune AI music recommendation system.\n")
    
    try:
        result = asyncio.run(demonstrate_unified_interface())
        print(f"\nüéâ Demo Status: {result['status']}")
        print("\n‚úÖ All examples completed successfully!")
        print("üöÄ Ready for production deployment with real GCP credentials")
        
    except Exception as e:
        print(f"\n‚ùå Demo error: {e}")